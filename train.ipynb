{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atul Anant (MDS202314) @ AML ASSIGNMENT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path: str) -> pd.DataFrame:    \n",
    "    df = pd.read_csv(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_labels(df: pd.DataFrame, feature_col: str = \"message\", label_col: str = \"label\"):    \n",
    "    X = df[feature_col]\n",
    "    y = df[label_col]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, y_train): \n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, X, y) -> float:\n",
    "    return model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_predictions(y_true, y_pred, title: str = \"\"):\n",
    "    print(f\"\\n=== Evaluation: {title} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, X_val, y_val) -> float:\n",
    "    \"\"\"\n",
    "    Validate the model on validation data and return the accuracy. \n",
    "    Returns  (float) Accuracy on the validation set.\n",
    "    \"\"\"\n",
    "    return model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_val, y_val):\n",
    "    # Fit on train\n",
    "    model = fit_model(model, X_train, y_train)\n",
    "    \n",
    "    # Score on train\n",
    "    train_acc = score_model(model, X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    \n",
    "    # Score on validation\n",
    "    val_acc = score_model(model, X_val, y_val)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    # Evaluate\n",
    "    print(f\"\\n=== Model: {type(model.steps[-1][1]).__name__} ===\")  \n",
    "    print(\"--- Train Evaluation ---\")\n",
    "    evaluate_model_predictions(y_train, y_train_pred, title=\"Train\")\n",
    "    \n",
    "    print(\"--- Validation Evaluation ---\")\n",
    "    evaluate_model_predictions(y_val, y_val_pred, title=\"Validation\")\n",
    "    \n",
    "    print(f\"Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    return model, (train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_hyperparams(model, param_grid, X_train, y_train, X_val, y_val):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(\"\\n=== Hyperparameter Tuning ===\")\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(f\"Best CV score: {best_score:.4f}\")\n",
    "\n",
    "    # Evaluate best_model on the validation set\n",
    "    val_acc = best_model.score(X_val, y_val)\n",
    "    print(f\"Validation Accuracy of best model: {val_acc:.4f}\")\n",
    "    \n",
    "    return best_model, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_benchmark_models_on_test(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Scores three  benchmark models on the test set, prints their accuracies,\n",
    "    and selects the best one based on test accuracy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models : dict\n",
    "        A dictionary of {model_name: fitted_model, ...} \n",
    "        e.g. {\n",
    "           'naive_bayes': nb_pipeline,\n",
    "           'log_reg': lr_pipeline,\n",
    "           'svm': svm_pipeline\n",
    "        }\n",
    "    X_test, y_test : test data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_model_name : str\n",
    "        Name of the best-performing model.\n",
    "    best_accuracy : float\n",
    "        Accuracy of the best-performing model.\n",
    "    \"\"\"\n",
    "    best_model_name = None\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"\\n=== {model_name.upper()} on Test Set ===\")\n",
    "        print(\"Test Accuracy:\", test_acc)\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "        if test_acc > best_accuracy:\n",
    "            best_accuracy = test_acc\n",
    "            best_model_name = model_name\n",
    "    \n",
    "    print(f\"\\nBest model on Test Set: {best_model_name} with accuracy={best_accuracy:.4f}\")\n",
    "    return best_model_name, best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Train & Evaluate: Naive Bayes ==========\n",
      "\n",
      "=== Model: MultinomialNB ===\n",
      "--- Train Evaluation ---\n",
      "\n",
      "=== Evaluation: Train ===\n",
      "Accuracy: 0.9773135669362084\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      3854\n",
      "           1       1.00      0.83      0.91       598\n",
      "\n",
      "    accuracy                           0.98      4452\n",
      "   macro avg       0.99      0.92      0.95      4452\n",
      "weighted avg       0.98      0.98      0.98      4452\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3854    0]\n",
      " [ 101  497]]\n",
      "--- Validation Evaluation ---\n",
      "\n",
      "=== Evaluation: Validation ===\n",
      "Accuracy: 0.9640933572710951\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       483\n",
      "           1       1.00      0.73      0.84        74\n",
      "\n",
      "    accuracy                           0.96       557\n",
      "   macro avg       0.98      0.86      0.91       557\n",
      "weighted avg       0.97      0.96      0.96       557\n",
      "\n",
      "Confusion Matrix:\n",
      " [[483   0]\n",
      " [ 20  54]]\n",
      "Train Accuracy: 0.9773, Validation Accuracy: 0.9641\n",
      "\n",
      "========== Train & Evaluate: Logistic Regression ==========\n",
      "\n",
      "=== Model: LogisticRegression ===\n",
      "--- Train Evaluation ---\n",
      "\n",
      "=== Evaluation: Train ===\n",
      "Accuracy: 0.9674303683737646\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      3854\n",
      "           1       0.99      0.77      0.86       598\n",
      "\n",
      "    accuracy                           0.97      4452\n",
      "   macro avg       0.98      0.88      0.92      4452\n",
      "weighted avg       0.97      0.97      0.97      4452\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3848    6]\n",
      " [ 139  459]]\n",
      "--- Validation Evaluation ---\n",
      "\n",
      "=== Evaluation: Validation ===\n",
      "Accuracy: 0.9658886894075404\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       483\n",
      "           1       1.00      0.74      0.85        74\n",
      "\n",
      "    accuracy                           0.97       557\n",
      "   macro avg       0.98      0.87      0.92       557\n",
      "weighted avg       0.97      0.97      0.96       557\n",
      "\n",
      "Confusion Matrix:\n",
      " [[483   0]\n",
      " [ 19  55]]\n",
      "Train Accuracy: 0.9674, Validation Accuracy: 0.9659\n",
      "\n",
      "========== Train & Evaluate: SVM ==========\n",
      "\n",
      "=== Model: SVC ===\n",
      "--- Train Evaluation ---\n",
      "\n",
      "=== Evaluation: Train ===\n",
      "Accuracy: 0.9970799640610961\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3854\n",
      "           1       1.00      0.98      0.99       598\n",
      "\n",
      "    accuracy                           1.00      4452\n",
      "   macro avg       1.00      0.99      0.99      4452\n",
      "weighted avg       1.00      1.00      1.00      4452\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3854    0]\n",
      " [  13  585]]\n",
      "--- Validation Evaluation ---\n",
      "\n",
      "=== Evaluation: Validation ===\n",
      "Accuracy: 0.9802513464991023\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       483\n",
      "           1       1.00      0.85      0.92        74\n",
      "\n",
      "    accuracy                           0.98       557\n",
      "   macro avg       0.99      0.93      0.95       557\n",
      "weighted avg       0.98      0.98      0.98       557\n",
      "\n",
      "Confusion Matrix:\n",
      " [[483   0]\n",
      " [ 11  63]]\n",
      "Train Accuracy: 0.9971, Validation Accuracy: 0.9803\n",
      "\n",
      "=== NAIVE_BAYES on Test Set ===\n",
      "Test Accuracy: 0.9658886894075404\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       482\n",
      "           1       1.00      0.75      0.85        75\n",
      "\n",
      "    accuracy                           0.97       557\n",
      "   macro avg       0.98      0.87      0.92       557\n",
      "weighted avg       0.97      0.97      0.96       557\n",
      "\n",
      "Confusion Matrix:\n",
      " [[482   0]\n",
      " [ 19  56]]\n",
      "\n",
      "=== LOG_REG on Test Set ===\n",
      "Test Accuracy: 0.9694793536804309\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       482\n",
      "           1       1.00      0.77      0.87        75\n",
      "\n",
      "    accuracy                           0.97       557\n",
      "   macro avg       0.98      0.89      0.93       557\n",
      "weighted avg       0.97      0.97      0.97       557\n",
      "\n",
      "Confusion Matrix:\n",
      " [[482   0]\n",
      " [ 17  58]]\n",
      "\n",
      "=== SVM on Test Set ===\n",
      "Test Accuracy: 0.9784560143626571\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       482\n",
      "           1       1.00      0.84      0.91        75\n",
      "\n",
      "    accuracy                           0.98       557\n",
      "   macro avg       0.99      0.92      0.95       557\n",
      "weighted avg       0.98      0.98      0.98       557\n",
      "\n",
      "Confusion Matrix:\n",
      " [[482   0]\n",
      " [ 12  63]]\n",
      "\n",
      "Best model on Test Set: svm with accuracy=0.9785\n",
      "\n",
      "=== Best model on Test is 'svm' with test accuracy=0.9785 ===\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load train, validation, test data\n",
    "    train_df = load_csv_data(r\"D:\\sem4\\Aml\\Assignment1\\data_splits\\train.csv\")\n",
    "    val_df   = load_csv_data(r\"D:\\sem4\\Aml\\Assignment1\\data_splits\\val.csv\")\n",
    "    test_df  = load_csv_data(r\"D:\\sem4\\Aml\\Assignment1\\data_splits\\test.csv\")\n",
    "\n",
    "    train_df.dropna(subset=[\"message\"], inplace=True)\n",
    "    val_df.dropna(subset=[\"message\"], inplace=True)\n",
    "    test_df.dropna(subset=[\"message\"], inplace=True)\n",
    "\n",
    "    # Prepare X, y\n",
    "    X_train, y_train = prepare_features_labels(train_df, label_col=\"label\", feature_col=\"message\")\n",
    "    X_val,   y_val   = prepare_features_labels(val_df,   label_col=\"label\", feature_col=\"message\")\n",
    "    X_test,  y_test  = prepare_features_labels(test_df,  label_col=\"label\", feature_col=\"message\")\n",
    "\n",
    "    # Define three benchmark pipelines\n",
    "    nb_pipeline = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\",   MultinomialNB())\n",
    "    ])\n",
    "\n",
    "    lr_pipeline = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\",   LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "    svm_pipeline = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\",   SVC())\n",
    "    ])\n",
    "\n",
    "    # Train + Score + Evaluate for each\n",
    "    print(\"\\n========== Train & Evaluate: Naive Bayes ==========\")\n",
    "    nb_model, (nb_train_acc, nb_val_acc) = train_and_evaluate(nb_pipeline, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    print(\"\\n========== Train & Evaluate: Logistic Regression ==========\")\n",
    "    lr_model, (lr_train_acc, lr_val_acc) = train_and_evaluate(lr_pipeline, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    print(\"\\n========== Train & Evaluate: SVM ==========\")\n",
    "    svm_model, (svm_train_acc, svm_val_acc) = train_and_evaluate(svm_pipeline, X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n",
    "    # Evaluate all three on the test set\n",
    "    all_models = {\n",
    "        'naive_bayes': nb_model,\n",
    "        'log_reg': lr_model,\n",
    "        'svm': svm_model\n",
    "    }\n",
    "\n",
    "    best_model_name, best_test_acc = score_benchmark_models_on_test(all_models, X_test, y_test)\n",
    "\n",
    "    print(f\"\\n=== Best model on Test is '{best_model_name}' with test accuracy={best_test_acc:.4f} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
